### Intro to Hadoop
![traditional vs Hadoop](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop1.PNG)

>Hadoop is the solution to complexities raised due to use of distributed systems.

> Four Key characteristics are:
1. Economical(any computer can be used)
2. Reliable(rseistant to hard ware failure )
3. Scalable(follows both horizontal and vertical scaling) 
4. Flexible(flexibilty to use of structured/unstructured data)

![Hadoop Environment](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/Hadoop2.PNG)

>here program goes into data instead of vice versa(conventional approach(rdbms)-overloading)

> hadoop ecosystem comprises of 12 components(which can be classified under Data Ingestion, Data Analysis, Data Processing, Data Exploration, Workflow System, No SQl) 

![Hadoop Stages](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/Hadoop3.PNG)

### analyze BigData With Hadoop

*launching functional Hadoop cluster using Amazon EMR*

>added a S3 bucket in config file of the cluster

![](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop2.1.PNG)

>added necessary steps to be executed *defining schema and table for sample log data in S3*

![](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop2.2.PNG)
> Macro Instances

![macro instances](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop2.3.PNG)

>OS_request *analyzed data using a HiveQl script and wrote results back to S3*

![](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop2.4.PNG)
![](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop2.5.PNG)

> Terminating

![Terminating](https://github.com/agsrc/dist-sys-practice/blob/master/Screenshots%201st%20Report/hadoop2.7.PNG)
![]()


