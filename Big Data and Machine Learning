text here
Video:
    Hadoop Introduction
    hadoop: framework for distributed process of large data.
    Four phase in data process: ingest, process,analyze, access.
    1. data are transfer to Hadoop from various data sources such as local files, relational dtabases, systems.
    2. data is stored and processed
    3. data is processing with processing models.
    4.  data is accessible to users.( Web interface or something else)
    Four key advantange: reliable, economical, scalable, flexible
    12 key components in Hadoop ecosystem:
        Data ingestion: sqoop ( transfer data between hadoop and relational databses servers.
        , flume (event data, streaming data).  
        Dara process: Hadoop mapReduce, Spark, Yarn, HDFS( for storing data. HBase, NoSQL..
        data analysis: pig, impala, hive.
        data access: cloudera search, hue.
        other: Oozie( workflow or coordination system. manage the Hadoop jobs.)
        Hbase( random, real-time, read/write access to your data).     
    Distributed system four key problems: bandwith limit, application complexity, high chance failure.

QwikLab:
   First, create a S3 bucket which is used to store the sample data and output.
   second, create a cluster which is used to process data.
   Thirs, assign jobs to the cluster.
        create hive table.
        read files ( ingest), write the results to the Hive table ( store)
        submit a hiveQL to retrive something from the hive table. ( process).
        write the query results to S3 bucket.( access).
        download the results to my computer.
   Learned how to launch a cluster, create a S3 bucket. And get familiar with the process in which the 
   Amazon EMR use the hadoop to process big data.
  
