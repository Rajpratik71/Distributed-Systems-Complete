text here
Video:
    Hadoop Introduction
    hadoop: framework for distributed process of large data.
    Four phase in data process: ingest, process,analyze, access.
    1. data are transfer to Hadoop from various data sources such as local files, relational dtabases, systems.
    2. data is stored and processed
    3. data is processing with processing models.
    4.  data is accessible to users.( Web interface or something else)
    Four key advantange: reliable, economical, scalable, flexible
    12 key components in Hadoop ecosystem:
        Data ingestion: sqoop ( transfer data between hadoop and relational databses servers.
        , flume (event data, streaming data).  
        Dara process: Hadoop mapReduce, Spark, Yarn, HDFS( for storing data. HBase, NoSQL..
        data analysis: pig, impala, hive.
        data access: cloudera search, hue.
        other: Oozie( workflow or coordination system. manage the Hadoop jobs.)
        Hbase( random, real-time, read/write access to your data).     
    Distributed system four key problems: bandwith limit, application complexity, high chance failure.

QwikLab:
   First, create a S3 bucket which is used to store the sample data and output.
   second, create a cluster which is used to process data.
   Thirs, assign jobs to the cluster.
        create hive table.
        read files ( ingest), write the results to the Hive table ( store)
        submit a hiveQL to retrive something from the hive table. ( process).
        write the query results to S3 bucket.( access).
        download the results to my computer.
   Learned how to launch a cluster, create a S3 bucket. And get familiar with the process in which the 
   Amazon EMR use the hadoop to process big data.


Intermediate Level:
Data storage
    QwikLab: Intro to S3 
    I have do this lab in thel beginner level of "Cloud Web Application".
    QwikLab: Intro to Amazon Redshift
    This lab step me through
        1. the launching of a Amazon Redshift
        2. Connecting the Pgweb to the Redshift cluster such that we can communicate with the Redshift cluster.
        3. Create a table in the Redshift cluster.
        4. load the data from an Amazon S3 bucket to the cluster.( by SQL query)
        5. Make some queries from the Pgweb to the Redshift.
    
        
